# Plan: Replace Static Provider Limits with models.dev Registry + Precise Per-Provider Limits

## Summary

Replace the hardcoded `PROVIDER_IMAGE_LIMITS` in `src/types.ts` with a comprehensive, research-backed registry. Also save the models.dev API data locally for reference, and gitignore it.

## Research Findings: Verified Provider Image Limits

| Provider                      | Per-Image Limit (base64)                                      | Max Dimensions           | Max Images/Req          | Auto-Resize?                     | Source                                                                                                    |
| ----------------------------- | ------------------------------------------------------------- | ------------------------ | ----------------------- | -------------------------------- | --------------------------------------------------------------------------------------------------------- |
| **Anthropic**                 | **5 MB** (API), 10 MB (web)                                   | 8000x8000 px             | 100 (≤20 for >2000px)   | No, rejects                      | [Anthropic Vision docs FAQ](https://platform.claude.com/docs/en/build-with-claude/vision)                 |
| **OpenAI**                    | **20 MB**                                                     | 2048x2048 (auto-resized) | 50                      | Yes (longest→2048, shortest→768) | Azure/OpenAI docs                                                                                         |
| **Azure OpenAI**              | **20 MB**                                                     | Same as OpenAI           | 50                      | Yes                              | [Azure docs](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/quotas-limits)                     |
| **Google Gemini (AI Studio)** | **20 MB request** / 7 MB inline                               | 3072x3072 (auto-resized) | 3,000                   | Yes                              | [Vertex AI docs](https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro)        |
| **Google Vertex AI**          | **7 MB** inline                                               | Same                     | 3,000                   | Yes                              | Same                                                                                                      |
| **AWS Bedrock**               | **3.75 MB**                                                   | 8000x8000 px             | 20                      | No, rejects                      | [AWS Bedrock docs](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-call.html) |
| **Groq**                      | **4 MB** (base64), 20 MB (URL)                                | 33 megapixels            | 5                       | No                               | [Groq Vision docs](https://console.groq.com/docs/vision)                                                  |
| **Fireworks AI**              | **10 MB total** (base64), 5 MB/ea (URL)                       | Not documented           | 30                      | No                               | [Fireworks docs](https://docs.fireworks.ai/guides/querying-vision-language-models)                        |
| **Perplexity**                | **50 MB** (base64)                                            | Not documented           | Not documented          | Unknown                          | [Perplexity docs](https://docs.perplexity.ai/guides/image-attachments)                                    |
| **xAI (Grok)**                | **20 MB**                                                     | Not documented           | Unlimited (token-bound) | Unknown                          | [xAI docs](https://docs.x.ai/docs/models)                                                                 |
| **DeepSeek**                  | **~10 MB** (undocumented, community reports)                  | Not documented           | Not documented          | Unknown                          | Community reports                                                                                         |
| **Together AI**               | **Undocumented**                                              | Not documented           | Not documented          | Unknown                          | No official docs                                                                                          |
| **GitHub Copilot**            | **Undocumented** (likely proxies to upstream provider limits) | N/A                      | N/A                     | N/A                              | No public API docs                                                                                        |
| **OpenCode**                  | **Proxied** (uses upstream provider limits)                   | N/A                      | N/A                     | N/A                              | N/A                                                                                                       |

### Key Insight: base64 overhead matters

Our plugin sends images as base64 data URIs. Base64 adds ~33% overhead. So if a provider says "5 MB max image", the raw image data must be under ~3.75 MB for the base64-encoded payload to stay under 5 MB.

**However**, the limits above refer to the _decoded_ image size (before base64 encoding), not the wire payload size. Providers decode the base64 before checking size. Our `TARGET_MULTIPLIER` of 0.7 already accounts for this.

### models.dev API

The models.dev API has **no image size limit fields**. It only provides:

- Model IDs, names, families
- Token limits (context, output)
- Modalities (input/output: text, image, audio, video, pdf)
- Cost info

Still useful as a reference for which models support image input, but cannot replace our manual limit registry.

## Plan

### 1. Save models.dev data and gitignore it

- Fetch `https://models.dev/api.json` and save to `models.dev.json` in the repo root
- Add `models.dev.json` to `.gitignore`

### 2. Update `PROVIDER_IMAGE_LIMITS` in `src/types.ts`

Replace the current hardcoded limits with verified values (with source URLs as comments):

```ts
export const PROVIDER_IMAGE_LIMITS: Record<string, number> = {
	// Anthropic: 5 MB per image (API). Rejects oversized.
	// https://platform.claude.com/docs/en/build-with-claude/vision (FAQ)
	anthropic: 5 * 1024 * 1024,
	'anthropic-beta': 5 * 1024 * 1024,

	// AWS Bedrock: 3.75 MB per image. Rejects oversized.
	// https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-call.html
	bedrock: 3.75 * 1024 * 1024,
	'aws-bedrock': 3.75 * 1024 * 1024,

	// OpenAI: 20 MB per image. Auto-resizes to 2048x2048.
	// https://learn.microsoft.com/en-us/azure/ai-foundry/openai/quotas-limits
	openai: 20 * 1024 * 1024,

	// Azure OpenAI: 20 MB per image. Same as OpenAI.
	azure: 20 * 1024 * 1024,
	'azure-openai': 20 * 1024 * 1024,

	// Google Gemini: 7 MB inline per image. Auto-resizes to ~3072x3072.
	// https://docs.cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro
	google: 7 * 1024 * 1024,
	'google-vertex': 7 * 1024 * 1024,
	vertex: 7 * 1024 * 1024,

	// Groq: 4 MB for base64, 20 MB for URL. We send base64.
	// https://console.groq.com/docs/vision
	groq: 4 * 1024 * 1024,

	// Fireworks AI: 10 MB total base64, 5 MB per URL.
	// https://docs.fireworks.ai/guides/querying-vision-language-models
	fireworks: 10 * 1024 * 1024,

	// Perplexity: 50 MB base64.
	// https://docs.perplexity.ai/guides/image-attachments
	perplexity: 50 * 1024 * 1024,

	// xAI (Grok): 20 MB per image.
	// https://docs.x.ai/docs/models
	xai: 20 * 1024 * 1024,

	// DeepSeek: ~10 MB (community consensus, not officially documented).
	deepseek: 10 * 1024 * 1024,

	// Together AI: Not documented. Conservative 20 MB.
	together: 20 * 1024 * 1024,
	'together-ai': 20 * 1024 * 1024,

	// Default for unknown providers (conservative)
	default: 5 * 1024 * 1024,
}
```

Note: `github-copilot`, `copilot`, and `opencode` are **removed** from the static map. They are proxy providers — we resolve their limits dynamically (see step 3).

### 3. Add smart proxy provider resolution in `src/image-processor.ts`

The transform hook provides both `providerID` and `modelID` on every message. For proxy providers (github-copilot, opencode, copilot), we can infer the upstream provider from the model ID:

```ts
// Model ID prefix -> upstream provider mapping
const MODEL_PREFIX_TO_PROVIDER: Record<string, string> = {
	claude: 'anthropic',
	gpt: 'openai',
	o1: 'openai',
	o3: 'openai',
	o4: 'openai',
	gemini: 'google',
	grok: 'xai',
	deepseek: 'deepseek',
	llama: 'groq', // common default, but could be together/fireworks
	mixtral: 'groq',
	qwen: 'fireworks',
}

const PROXY_PROVIDERS = new Set(['github-copilot', 'copilot', 'opencode'])

export function getProviderLimit(providerID: string, modelID?: string): number {
	// If provider is a proxy, try to resolve from model ID
	if (PROXY_PROVIDERS.has(providerID) && modelID) {
		const resolved = resolveProviderFromModel(modelID)
		if (resolved && PROVIDER_IMAGE_LIMITS[resolved]) {
			return PROVIDER_IMAGE_LIMITS[resolved]
		}
	}
	return PROVIDER_IMAGE_LIMITS[providerID] || PROVIDER_IMAGE_LIMITS.default
}

function resolveProviderFromModel(modelID: string): string | undefined {
	const lower = modelID.toLowerCase()
	for (const [prefix, provider] of Object.entries(MODEL_PREFIX_TO_PROVIDER)) {
		if (lower.startsWith(prefix)) return provider
	}
	return undefined
}
```

Update `processImagePart` to accept `modelID` and pass it to `getProviderLimit`.

Update `index.ts` to extract `modelID` from the message info alongside `providerID` and pass it through.

### 4. Update tests

- Update `image-processor.test.ts` for new limit values (google 7 MB, add groq/bedrock tests)
- Add tests for `getProviderLimit` with proxy resolution (e.g., `getProviderLimit('github-copilot', 'claude-sonnet-4-5')` should return Anthropic's 5 MB)
- Add tests for `getProviderLimit` fallback when model ID is unknown

### 5. Verify

Run `bun fmt && bun lint && bun typecheck && bun test:unit`.

## Files to Modify

- `.gitignore` — add `models.dev.json`
- `models.dev.json` — new file (fetched data, gitignored)
- `src/types.ts` — update `PROVIDER_IMAGE_LIMITS`, remove proxy providers, add `MODEL_PREFIX_TO_PROVIDER` and `PROXY_PROVIDERS`
- `src/image-processor.ts` — update `getProviderLimit` to accept `modelID`, add `resolveProviderFromModel`, update `processImagePart` signature
- `src/index.ts` — extract `modelID` from message info alongside `providerID`, pass to `processImagePart`
- `tests/unit/image-processor.test.ts` — update test expectations, add proxy resolution tests
